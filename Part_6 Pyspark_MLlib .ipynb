{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcff85b",
   "metadata": {},
   "source": [
    "### Machine Learning Library (MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2a0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ced4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 12:50:52 WARN Utils: Your hostname, dell-5570 resolves to a loopback address: 127.0.1.1; using 10.189.123.35 instead (on interface wlp0s20f3)\n",
      "22/11/20 12:50:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 12:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36a58b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.189.123.35:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0160536cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e9dfd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "df = spark.read.csv(\"data/paysim.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c652479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2336f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|   type| amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1|PAYMENT|9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1|PAYMENT|1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "+----+-------+-------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaea7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ee149a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7,0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d76243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set lenght: 4453616 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=========================>                              (9 + 11) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set lenght: 1909004 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"train set lenght: {train.count()} records\")\n",
    "print(f\"test set lenght: {test.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51f36850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------+--------------+-------+\n",
      "|   type|amount|oldbalanceOrg|newbalanceOrig|isFraud|\n",
      "+-------+------+-------------+--------------+-------+\n",
      "|CASH_IN|  5.66|   5061561.06|    5061566.72|      0|\n",
      "|CASH_IN|  14.4|1.143460813E7| 1.143462253E7|      0|\n",
      "+-------+------+-------------+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91da05",
   "metadata": {},
   "source": [
    "## Dtypes\n",
    "\n",
    "* in this dataset, any columns of type string as a categotical feature, but sometimes we might have numeric features we want treated as categotical or vice versa. We'll need to carefully identify which columns are numeric and which are categotical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f0e4144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('type', 'string'),\n",
       " ('amount', 'double'),\n",
       " ('oldbalanceOrg', 'double'),\n",
       " ('newbalanceOrig', 'double'),\n",
       " ('isFraud', 'int')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd3f89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = [x for (x, dataType) in train.dtypes if dataType ==\"string\"]\n",
    "numCols = [x for (x, dataType) in train.dtypes if ((dataType ==\"double\") & (x != \"isFraud\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75956957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
      "['type']\n"
     ]
    }
   ],
   "source": [
    "print(numCols)\n",
    "print(catCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "691b5f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=====>                                                  (2 + 18) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(type)|\n",
      "+-----------+\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.agg(F.countDistinct(\"type\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0cd8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:>                                                       (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/11/20 15:49:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:==>                                                     (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    type|  count|\n",
      "+--------+-------+\n",
      "|TRANSFER| 372954|\n",
      "| CASH_IN| 979563|\n",
      "|CASH_OUT|1566092|\n",
      "| PAYMENT|1506065|\n",
      "|   DEBIT|  28942|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:==============================>                         (11 + 9) / 20]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "149c28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (OneHotEncoder, StringIndexer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e030c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_indexer = [StringIndexer(inputCol = x, outputCol = x + \"_StringIndexer\", handleInvalid = \"skip\") for x in catCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "adb23b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_09cfc459d60f]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "780d7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = [OneHotEncoder(\n",
    "inputCols = [f\"{x}_StringIndexer\" for x in catCols],\n",
    "outputCols = [f\"{x}_OneHotEncoder\" for x in catCols],\n",
    ")\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b15ec7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_fbae5d1b0f57]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aca4b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02a44fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput  = [x for x in numCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c0b6a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput += [f\"{x}_oneHotEncoder\" for x in catCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f9fb6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'oldbalanceOrg', 'newbalanceOrig', 'type_oneHotEncoder']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assemblerInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e09b4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(\n",
    "inputCols = assemblerInput, outputCol = \"VectorAssembmler_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1c105d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "stages +=string_indexer\n",
    "stages +=one_hot_encoder\n",
    "stages +=[vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aaa898ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_09cfc459d60f,\n",
       " OneHotEncoder_fbae5d1b0f57,\n",
       " VectorAssembler_b440cd865c97]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4013e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:================>                                       (6 + 14) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 0 ns, total: 20.5 ms\n",
      "Wall time: 2.67 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "#pp_df =  model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ce6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/20 18:04:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2934359 ms exceeds timeout 120000 ms\n",
      "22/11/20 18:04:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9bca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
